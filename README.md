# ğŸš€ LlamaRefine - Refining TinyLlama with Supervised Fine-Tuning and Human Preferences (DPO)

This repository contains a complete **notebook-based pipeline** for fine-tuning the [TinyLlama](https://huggingface.co/TinyLlama) language model using **Direct Preference Optimization (DPO)** and **Supervised Fine-Tuning (SFT)** with **LoRA adapters**. It leverages **Unsloth** and **TRL** for efficient and scalable training on low-resource hardware.

---

## ğŸ“Œ What is DPO (Direct Preference Optimization)?

**DPO** is a lightweight reinforcement learning technique that allows LLMs to learn **preferences between good and bad responses** using only ranked pairs â€” without needing a separate reward model or complex PPO setup.

> ğŸ“¥ Input: Prompt + Chosen response + Rejected response  
> ğŸ¯ Goal: Make the model assign higher likelihood to the "chosen" over the "rejected"

This approach directly aligns model behavior with **human preferences** and is highly efficient.

---

## ğŸ§  Final Unified Model Capabilities

After the full pipeline, your final model can:

- ğŸ§  Retain **TinyLlamaâ€™s original knowledge** (pretraining)
- ğŸ“ Follow instructions and tasks with **SFT**
- ğŸ“Š Prefer better responses with **DPO preference learning**

âœ… You get a **single unified model**, merged and ready for inference, without needing LoRA adapters during runtime.

---

## ğŸš€ What This Project Does

- âœ… Fine-tunes TinyLlama using helpful outputs (SFT)
- âœ… Optimizes the model to prefer better responses (DPO)
- âœ… Uses LoRA for low-memory training
- âœ… Merges everything into one deployable model

---

## ğŸ”§ Tech Stack
- ğŸ¤– TinyLlama (base model)
- âš¡ Unsloth (fast fine-tuning)
- ğŸ” TRL (Transformers RL) (DPO implementation)
- ğŸ“¦ PEFT (LoRA adapters)
- ğŸ¤— Hugging Face ecosystem (datasets, transformers, accelerate)

---

## ğŸ“’ Notebooks

| Notebook | Description |
|----------|-------------|
| `01_SFT.ipynb` | Performs Supervised Fine-Tuning (SFT) using LoRA |
| `02_DPO.ipynb` | Performs Direct Preference Optimization (DPO) and merges adapters |
| `03_Inference.py` | Runs inference using the final merged model |

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ 01_SFT.ipynb            # SFT with LoRA
â”œâ”€â”€ 02_DPO.ipynb            # DPO training + merging adapters
â”œâ”€â”€ 03_Inference.ipynb      # Inference from final model
â””â”€â”€ README.md               # This file
```

---

## ğŸ“¦ Dataset

- **Used for SFT**: Custom dataset from [`NvidiaDocumentationQandApairs.csv`](https://www.kaggle.com/datasets/gondimalladeepesh/nvidia-documentation-question-and-answer-pairs), consisting of high-quality Q&A pairs extracted from NVIDIA technical documentation.
- **Used for DPO**: [`yitingxie/rlhf-reward-datasets`](https://huggingface.co/datasets/yitingxie/rlhf-reward-datasets)  
  Format: Each entry contains a prompt, a chosen response (preferred), and a rejected response.

---

## ğŸ‹ï¸â€â™‚ï¸ Training Pipeline

1. ğŸ§ª Supervised Fine-Tuning (SFT)

```bash
run: 01_SFT.ipynb
```
- Fine-tunes TinyLlama with helpful responses using LoRA.
- Saves the LoRA adapter (sft_adapter).

2. ğŸ¯ Direct Preference Optimization (DPO) and ğŸ”€ Merge Adapters

```bash
Run: 02_DPO.ipynb
```

- Uses sft_adapter as a base.
- Trains the model to prefer "chosen" over "rejected" outputs.
- Produces a dpo_adapter.
- Merges base model + SFT adapter + DPO adapter into one final model.


3. ğŸ¤– Inference

```bash
Run: Inference.py
```
Test the final model with custom prompts.

---

## ğŸ§  Model Behavior
After training, the final model:
- Understands natural prompts thanks to SFT
- Generates preferred and high-quality responses thanks to DPO
- Runs standalone (LoRA adapters merged)

---

## ğŸ’¡ Use Case
This project is ideal for:

- Creating lightweight RLHF-style LLMs without PPO (Proximal Policy Optimization)
- Instruction-following LLMs tuned for preference-aligned outputs
- Prototyping custom assistants, chatbots, or tutor bots using small models

---

## ğŸ§ª Proof of Concept (POC)

The following screenshots show the directory structure , model behavior before and after DPO preference tuning:
![image](https://github.com/user-attachments/assets/2238898a-f5a3-4572-8b08-38f9bcfb7b2d)


![image](https://github.com/user-attachments/assets/b4bba7a7-3efc-4aa2-9f52-cb02f3fdcc9c)

---

## ğŸ“¤ Future Enhancements
- âœ… Upload final model to Hugging Face Hub
- ğŸ“ˆ Evaluate model using MT-Bench or AlpacaEval
- ğŸ’¬ Add Gradio/Streamlit chatbot UI
- ğŸ”— Integrate with LangChain or RAG pipeline
